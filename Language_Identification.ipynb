{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8f656a",
   "metadata": {},
   "source": [
    "## Language Identification  \n",
    "The objective of this project is to implement a machine learning-based Language Identification System that can automatically detect the language of a given sentence. The focus is on identifying four languages: English, Urdu, Russian, and Chinese.\n",
    "\n",
    "The dataset was obtained from the Hugging Face Dataset Willi 2018 it contains 235 different languages and I filtered my desired languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0222b38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train and test data loaded successfully.\n",
      "Original train shape: (117500, 2)\n",
      "Original test shape: (117500, 2)\n",
      "\n",
      "Filtering DataFrames...\n",
      "Train data filtered to 2000 rows.\n",
      "Test data filtered to 2000 rows.\n",
      "Converting numerical labels to language names...\n",
      "\n",
      "Saving filtered and mapped train data to 'train_filtered_languages.parquet'...\n",
      "Saving filtered and mapped test data to 'test_filtered_languages.parquet'...\n",
      "\n",
      "Process complete!\n",
      "New train file saved: train_filtered_languages.parquet\n",
      "New test file saved: test_filtered_languages.parquet\n",
      "\n",
      "--- Sample of Filtered Train DataFrame (first 5 rows) ---\n",
      "                                              sentence language\n",
      "44   برقی بار (electric charge) تمام زیرجوہری ذرات ...     Urdu\n",
      "67   胡赛尼本人和小说的主人公阿米尔一样，都是出生在阿富汗首都喀布尔，少年时代便离开了这个国家。胡...  Chinese\n",
      "219  Занимает пятое место в диптихе автокефальных п...  Russian\n",
      "269  In 1978 Johnson was awarded an American Instit...  English\n",
      "306  Bussy-Saint-Georges has built its identity on ...  English\n",
      "\n",
      "--- Value counts for 'language' in Filtered Train DataFrame ---\n",
      "language\n",
      "Urdu       500\n",
      "Chinese    500\n",
      "Russian    500\n",
      "English    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Sample of Filtered Test DataFrame (first 5 rows) ---\n",
      "                                              sentence language\n",
      "42   大都会区有它自己的当地路边快餐口味，包括瓦达帕夫（蓬松面包劈开一半，填入锅贴）、潘尼普里（油...  Chinese\n",
      "43   16 апреля 2009 года в Шатойском районе произош...  Russian\n",
      "68   Anton (or Antonius) Maria Schyrleus (also Schy...  English\n",
      "184  اس وقت کے گورنر ڈی وٹ کلنٹن نے دریائے ہڈسن کو ...     Urdu\n",
      "193  近幾年來，由於氣候變遷對人類帶來的警訊，讓各國政府紛紛思考如何減碳節能。為減少對化石能源的依...  Chinese\n",
      "\n",
      "--- Value counts for 'language' in Filtered Test DataFrame ---\n",
      "language\n",
      "Chinese    500\n",
      "Russian    500\n",
      "English    500\n",
      "Urdu       500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the numerical labels you want to keep\n",
    "desired_label_numbers = [41, 179, 30, 24]\n",
    "\n",
    "# Define the mapping from numerical labels to language names\n",
    "label_to_language_map = {\n",
    "    41: 'English',\n",
    "    179: 'Chinese',\n",
    "    30: 'Urdu',\n",
    "    24: 'Russian'\n",
    "}\n",
    "\n",
    "# Define your input file paths (assuming they are in the current directory)\n",
    "train_input_file = 'train.parquet'\n",
    "test_input_file = 'test.parquet'\n",
    "\n",
    "# Define your output file names for the new filtered and mapped files\n",
    "train_output_file = 'train_filtered_languages.parquet'\n",
    "test_output_file = 'test_filtered_languages.parquet'\n",
    "\n",
    "# --- 1. Load the Parquet files ---\n",
    "\n",
    "df_train = pd.read_parquet(train_input_file)\n",
    "df_test = pd.read_parquet(test_input_file)\n",
    "print(\"Original train and test data loaded successfully.\")\n",
    "print(f\"Original train shape: {df_train.shape}\")\n",
    "print(f\"Original test shape: {df_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Filter the DataFrames by desired labels ---\n",
    "print(\"\\nFiltering DataFrames...\")\n",
    "df_train_filtered = df_train[df_train['label'].isin(desired_label_numbers)].copy()\n",
    "df_test_filtered = df_test[df_test['label'].isin(desired_label_numbers)].copy()\n",
    "\n",
    "print(f\"Train data filtered to {df_train_filtered.shape[0]} rows.\")\n",
    "print(f\"Test data filtered to {df_test_filtered.shape[0]} rows.\")\n",
    "\n",
    "# --- 3. Convert numerical labels to language names ---\n",
    "print(\"Converting numerical labels to language names...\")\n",
    "df_train_filtered['language'] = df_train_filtered['label'].map(label_to_language_map)\n",
    "df_test_filtered['language'] = df_test_filtered['label'].map(label_to_language_map)\n",
    "\n",
    "\n",
    "# Reorder columns if you want 'language' before 'sentence', for example\n",
    "df_train_filtered = df_train_filtered[['sentence', 'language']] # Keep original label too\n",
    "df_test_filtered = df_test_filtered[['sentence', 'language']]\n",
    "\n",
    "# --- 4. Save the new DataFrames to Parquet files ---\n",
    "print(f\"\\nSaving filtered and mapped train data to '{train_output_file}'...\")\n",
    "df_train_filtered.to_parquet(train_output_file, index=False) # index=False prevents writing the DataFrame index as a column\n",
    "\n",
    "print(f\"Saving filtered and mapped test data to '{test_output_file}'...\")\n",
    "df_test_filtered.to_parquet(test_output_file, index=False)\n",
    "\n",
    "print(\"\\nProcess complete!\")\n",
    "print(f\"New train file saved: {train_output_file}\")\n",
    "print(f\"New test file saved: {test_output_file}\")\n",
    "\n",
    "# Display a sample of the new DataFrames to verify\n",
    "print(\"\\n--- Sample of Filtered Train DataFrame (first 5 rows) ---\")\n",
    "print(df_train_filtered.head())\n",
    "print(\"\\n--- Value counts for 'language' in Filtered Train DataFrame ---\")\n",
    "print(df_train_filtered['language'].value_counts())\n",
    "\n",
    "print(\"\\n--- Sample of Filtered Test DataFrame (first 5 rows) ---\")\n",
    "print(df_test_filtered.head())\n",
    "print(\"\\n--- Value counts for 'language' in Filtered Test DataFrame ---\")\n",
    "print(df_test_filtered['language'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613bf027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your new files\n",
    "train_new_file = 'train_filtered_languages.parquet'\n",
    "test_new_file = 'test_filtered_languages.parquet'\n",
    "\n",
    "\n",
    "df_train = pd.read_parquet(train_new_file)\n",
    "df_test = pd.read_parquet(test_new_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c7e44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>大都会区有它自己的当地路边快餐口味，包括瓦达帕夫（蓬松面包劈开一半，填入锅贴）、潘尼普里（油...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16 апреля 2009 года в Шатойском районе произош...</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anton (or Antonius) Maria Schyrleus (also Schy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اس وقت کے گورنر ڈی وٹ کلنٹن نے دریائے ہڈسن کو ...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>近幾年來，由於氣候變遷對人類帶來的警訊，讓各國政府紛紛思考如何減碳節能。為減少對化石能源的依...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language\n",
       "0  大都会区有它自己的当地路边快餐口味，包括瓦达帕夫（蓬松面包劈开一半，填入锅贴）、潘尼普里（油...  Chinese\n",
       "1  16 апреля 2009 года в Шатойском районе произош...  Russian\n",
       "2  Anton (or Antonius) Maria Schyrleus (also Schy...  English\n",
       "3  اس وقت کے گورنر ڈی وٹ کلنٹن نے دریائے ہڈسن کو ...     Urdu\n",
       "4  近幾年來，由於氣候變遷對人類帶來的警訊，讓各國政府紛紛思考如何減碳節能。為減少對化石能源的依...  Chinese"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628506a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>برقی بار (electric charge) تمام زیرجوہری ذرات ...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>胡赛尼本人和小说的主人公阿米尔一样，都是出生在阿富汗首都喀布尔，少年时代便离开了这个国家。胡...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Занимает пятое место в диптихе автокефальных п...</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1978 Johnson was awarded an American Instit...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bussy-Saint-Georges has built its identity on ...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language\n",
       "0  برقی بار (electric charge) تمام زیرجوہری ذرات ...     Urdu\n",
       "1  胡赛尼本人和小说的主人公阿米尔一样，都是出生在阿富汗首都喀布尔，少年时代便离开了这个国家。胡...  Chinese\n",
       "2  Занимает пятое место в диптихе автокефальных п...  Russian\n",
       "3  In 1978 Johnson was awarded an American Instit...  English\n",
       "4  Bussy-Saint-Georges has built its identity on ...  English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19a570",
   "metadata": {},
   "source": [
    "### Data Preprocessing and cleaning\n",
    "Preprocessing is a crucial step in language detection to reduce noise and improve model performance. A custom clean_text function was implemented to clean each sentence by:\n",
    "\n",
    "•\tRemoving digits\n",
    "\n",
    "•\tRemoving punctuation and special characters\n",
    "\n",
    "•\tCollapsing multiple spaces into a single space\n",
    "\n",
    "•\tStripping leading/trailing whitespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1f0850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    برقی بار electric charge تمام زیرجوہری ذرات کی...\n",
      "1    胡赛尼本人和小说的主人公阿米尔一样都是出生在阿富汗首都喀布尔少年时代便离开了这个国家胡赛尼直...\n",
      "2    Занимает пятое место в диптихе автокефальных п...\n",
      "3    In Johnson was awarded an American Institute o...\n",
      "4    BussySaintGeorges has built its identity on a ...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[0-9]', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning and replace original sentence column\n",
    "df_train['sentence'] = df_train['sentence'].apply(clean_text)\n",
    "df_test['sentence'] = df_test['sentence'].apply(clean_text)\n",
    "\n",
    "# Optional: Preview cleaned data\n",
    "print(df_train['sentence'].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbd7c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>大都会区有它自己的当地路边快餐口味包括瓦达帕夫蓬松面包劈开一半填入锅贴潘尼普里油炸crêpe...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>апреля года в Шатойском районе произошёл бой м...</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anton or Antonius Maria Schyrleus also Schyrl ...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اس وقت کے گورنر ڈی وٹ کلنٹن نے دریائے ہڈسن کو ...</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>近幾年來由於氣候變遷對人類帶來的警訊讓各國政府紛紛思考如何減碳節能為減少對化石能源的依賴性有...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language\n",
       "0  大都会区有它自己的当地路边快餐口味包括瓦达帕夫蓬松面包劈开一半填入锅贴潘尼普里油炸crêpe...  Chinese\n",
       "1  апреля года в Шатойском районе произошёл бой м...  Russian\n",
       "2  Anton or Antonius Maria Schyrleus also Schyrl ...  English\n",
       "3  اس وقت کے گورنر ڈی وٹ کلنٹن نے دریائے ہڈسن کو ...     Urdu\n",
       "4  近幾年來由於氣候變遷對人類帶來的警訊讓各國政府紛紛思考如何減碳節能為減少對化石能源的依賴性有...  Chinese"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221855d",
   "metadata": {},
   "source": [
    "### Embeddings using TF-IDF\n",
    "To convert textual data into numeric form, TF-IDF (Term Frequency - Inverse Document Frequency) features were extracted. Since character-level patterns are more effective for language detection, character n-grams ranging from 1 to 5 were used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dbafc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use character-level n-grams (works best for language detection)\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 5), max_features=10000)\n",
    "\n",
    "# Fit on training set and transform both train and test\n",
    "X_train = vectorizer.fit_transform(df_train['sentence'])\n",
    "X_test = vectorizer.transform(df_test['sentence'])\n",
    "\n",
    "# Target labels\n",
    "y_train = df_train['language']\n",
    "y_test = df_test['language']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03256f",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation \n",
    "A Multinomial Naive Bayes model was selected for training due to its effectiveness and speed on text classification tasks.\n",
    "The model was evaluated using accuracy and a classification report, which includes precision, recall, and F1-score for each language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c154b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.991\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Chinese       1.00      0.98      0.99       500\n",
      "     English       0.97      1.00      0.98       500\n",
      "     Russian       1.00      1.00      1.00       500\n",
      "        Urdu       1.00      0.99      0.99       500\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize and train the model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer for future use\n",
    "import joblib  \n",
    "# Save the model\n",
    "joblib.dump(nb_model, 'language_detection_model.pkl')  \n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "# Load the model and vectorizer\n",
    "nb_model_loaded = joblib.load('language_detection_model.pkl')\n",
    "vectorizer_loaded = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc3165",
   "metadata": {},
   "source": [
    "### Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c4ba7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted language for the input text is: English\n"
     ]
    }
   ],
   "source": [
    "text= \"Hello, how are you?\"\n",
    "# Clean the input text\n",
    "cleaned_text = clean_text(text)\n",
    "# Transform the cleaned text into the same feature space\n",
    "X_input = vectorizer.transform([cleaned_text])\n",
    "# Predict the language\n",
    "predicted_language = nb_model.predict(X_input)\n",
    "print(f\"The predicted language for the input text is: {predicted_language[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "001a58b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted language for the input text is: Chinese\n"
     ]
    }
   ],
   "source": [
    "text = \"你好，你好吗？\"\n",
    "# Clean the input text \n",
    "cleaned_text = clean_text(text)\n",
    "# Transform the cleaned text into the same feature space\n",
    "X_input = vectorizer.transform([cleaned_text])\n",
    "# Predict the language\n",
    "predicted_language = nb_model.predict(X_input)\n",
    "print(f\"The predicted language for the input text is: {predicted_language[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee62ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted language for the example sentence is: English\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the loaded model\n",
    "def predict_language(sentence):\n",
    "    # Clean the input sentence\n",
    "    cleaned_sentence = clean_text(sentence)\n",
    "    # Transform the sentence using the loaded vectorizer\n",
    "    X_new = vectorizer_loaded.transform([cleaned_sentence])\n",
    "    # Predict the language\n",
    "    predicted_language = nb_model_loaded.predict(X_new)\n",
    "    return predicted_language[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12e9db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted language for the example sentence is: English\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "example_sentence = \"Hello, how are you?\"\n",
    "predicted_language = predict_language(example_sentence)\n",
    "print(f\"The predicted language for the example sentence is: {predicted_language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47174ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted language for the example sentence is: Chinese\n",
      "The predicted language for the example sentence is: Russian\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"你好，你好吗？\"\n",
    "predicted_language = predict_language(example_sentence)\n",
    "print(f\"The predicted language for the example sentence is: {predicted_language}\")\n",
    "# Example usage with a different sentence  \n",
    "example_sentence = \"Привет, как дела?\"\n",
    "predicted_language = predict_language(example_sentence)\n",
    "print(f\"The predicted language for the example sentence is: {predicted_language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17685ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
